import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Callable
import threading
from datetime import datetime

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS

from .vector_store import VectorStoreManager
from .file_watcher import FileSystemWatcher
from ..mcp.mcp_client import MCPFilesystemManager
from ..document_loaders.document_loader import DocumentLoaderManager
from ..config.settings import settings

logger = logging.getLogger(__name__)


class DynamicVectorStoreManager(VectorStoreManager):
    """åŠ¨æ€FAISSå‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, use_openai_embeddings: bool = False, 
                 enable_file_watching: bool = True,
                 enable_mcp: bool = True):
        """
        åˆå§‹åŒ–åŠ¨æ€å‘é‡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            use_openai_embeddings: æ˜¯å¦ä½¿ç”¨OpenAI embeddings
            enable_file_watching: æ˜¯å¦å¯ç”¨æ–‡ä»¶ç›‘æ§
            enable_mcp: æ˜¯å¦å¯ç”¨MCP
        """
        super().__init__(use_openai_embeddings)
        
        # åŠ¨æ€ç®¡ç†ç›¸å…³å±æ€§
        self.enable_file_watching = enable_file_watching
        self.enable_mcp = enable_mcp
        
        # æ–‡ä»¶ç›‘æ§å™¨
        self.file_watcher: Optional[FileSystemWatcher] = None
        
        # MCPç®¡ç†å™¨
        self.mcp_manager: Optional[MCPFilesystemManager] = None
        if self.enable_mcp:
            self.mcp_manager = MCPFilesystemManager()
        
        # æ–‡ä»¶çŠ¶æ€è·Ÿè¸ª
        self._file_document_mapping: Dict[str, List[str]] = {}  # æ–‡ä»¶è·¯å¾„ -> æ–‡æ¡£IDåˆ—è¡¨
        self._processing_files: Set[str] = set()  # æ­£åœ¨å¤„ç†çš„æ–‡ä»¶
        self._file_last_modified: Dict[str, float] = {}  # æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´
        
        # çº¿ç¨‹é”
        self._lock = threading.RLock()
        
        logger.info(f"åŠ¨æ€å‘é‡å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ– - æ–‡ä»¶ç›‘æ§: {enable_file_watching}, MCP: {enable_mcp}")
    
    async def initialize(self, store_name: str = "default", force_recreate: bool = False) -> FAISS:
        """åˆå§‹åŒ–åŠ¨æ€å‘é‡å­˜å‚¨"""
        logger.info("åˆå§‹åŒ–åŠ¨æ€å‘é‡å­˜å‚¨...")
        
        # åˆå§‹åŒ–MCP
        if self.mcp_manager:
            await self.mcp_manager.initialize()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨é‡å»ºå‘é‡å­˜å‚¨
        if settings.auto_rebuild_vector_store:
            logger.info("ğŸ”„ é…ç½®äº†è‡ªåŠ¨é‡å»ºå‘é‡å­˜å‚¨ï¼Œæ­£åœ¨é‡å»º...")
            force_recreate = True
        
        # åˆ›å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨ - ä½¿ç”¨åŠ¨æ€æ¨¡å¼çš„é€»è¾‘
        vector_store = await self._get_or_create_dynamic_vector_store(store_name, force_recreate)
        
        # å»ºç«‹æ–‡ä»¶åˆ°æ–‡æ¡£çš„æ˜ å°„
        await self._build_file_document_mapping()
        
        # å¯åŠ¨æ–‡ä»¶ç›‘æ§
        if self.enable_file_watching:
            await self._start_file_watching()
        
        logger.info("åŠ¨æ€å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
        return vector_store
    
    async def _get_or_create_dynamic_vector_store(self, store_name: str = "default", force_recreate: bool = False) -> FAISS:
        """è·å–æˆ–åˆ›å»ºåŠ¨æ€å‘é‡å­˜å‚¨ - ä½¿ç”¨ DOCUMENT_DIRECTORIES é…ç½®"""
        if force_recreate or not self._vector_store_exists(store_name):
            logger.info("åˆ›å»ºæ–°çš„åŠ¨æ€å‘é‡å­˜å‚¨...")
            await self._create_dynamic_vector_store()
            self.save_vector_store(store_name)
        else:
            logger.info("åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨...")
            self.load_vector_store(store_name)
        
        return self.vector_store
    
    async def _create_dynamic_vector_store(self) -> FAISS:
        """åˆ›å»ºåŠ¨æ€å‘é‡å­˜å‚¨ - ä» DOCUMENT_DIRECTORIES åŠ è½½æ–‡æ¡£"""
        logger.info("ä»é…ç½®çš„æ–‡æ¡£ç›®å½•åŠ è½½æ–‡æ¡£...")
        
        all_documents = []
        
        # éå†æ‰€æœ‰é…ç½®çš„æ–‡æ¡£ç›®å½•
        for directory_path in settings.document_directories:
            data_dir = Path(directory_path)
            
            if data_dir.exists():
                logger.info(f"æ­£åœ¨åŠ è½½ç›®å½•: {directory_path}")
                try:
                    documents = self.document_loader.load_documents_from_directory(str(data_dir))
                    all_documents.extend(documents)
                    logger.info(f"ä» {directory_path} åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                except Exception as e:
                    logger.error(f"åŠ è½½ç›®å½•å¤±è´¥ {directory_path}: {str(e)}")
            else:
                logger.warning(f"ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {directory_path}")
        
        if not all_documents:
            raise ValueError(f"æœªåœ¨é…ç½®çš„ç›®å½•ä¸­æ‰¾åˆ°å¯ç”¨çš„æ–‡æ¡£: {settings.document_directories}")
        
        logger.info(f"æ€»å…±åŠ è½½ {len(all_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µç”¨äºåˆ›å»ºå‘é‡å­˜å‚¨")
        logger.info(f"ä½¿ç”¨çš„æ–‡æ¡£ç›®å½•: {settings.document_directories}")
        
        try:
            self.vector_store = FAISS.from_documents(
                documents=all_documents,
                embedding=self.embeddings
            )
            logger.info("åŠ¨æ€å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")
            return self.vector_store
        except Exception as e:
            logger.error(f"åˆ›å»ºåŠ¨æ€å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            raise
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("æ¸…ç†åŠ¨æ€å‘é‡å­˜å‚¨èµ„æº...")
        
        # åœæ­¢æ–‡ä»¶ç›‘æ§
        if self.file_watcher:
            self.file_watcher.stop()
            self.file_watcher = None
        
        # æ¸…ç†MCPè¿æ¥
        if self.mcp_manager:
            await self.mcp_manager.cleanup()
        
        logger.info("åŠ¨æ€å‘é‡å­˜å‚¨èµ„æºæ¸…ç†å®Œæˆ")
    
    async def _build_file_document_mapping(self):
        """å»ºç«‹æ–‡ä»¶åˆ°æ–‡æ¡£çš„æ˜ å°„å…³ç³»"""
        logger.info("å»ºç«‹æ–‡ä»¶åˆ°æ–‡æ¡£çš„æ˜ å°„å…³ç³»...")
        
        if not self.vector_store:
            logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•å»ºç«‹æ˜ å°„")
            return
        
        with self._lock:
            self._file_document_mapping.clear()
            self._file_last_modified.clear()
            
            # éå†æ‰€æœ‰æ–‡æ¡£ï¼Œå»ºç«‹æ˜ å°„
            for doc_id, document in self.vector_store.docstore._dict.items():
                if hasattr(document, 'metadata') and 'source' in document.metadata:
                    source_path = document.metadata['source']
                    
                    if source_path not in self._file_document_mapping:
                        self._file_document_mapping[source_path] = []
                    
                    self._file_document_mapping[source_path].append(doc_id)
                    
                    # è®°å½•æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´
                    try:
                        file_path = Path(source_path)
                        if file_path.exists():
                            self._file_last_modified[source_path] = file_path.stat().st_mtime
                    except Exception as e:
                        logger.warning(f"æ— æ³•è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´ {source_path}: {str(e)}")
        
        logger.info(f"å»ºç«‹äº† {len(self._file_document_mapping)} ä¸ªæ–‡ä»¶çš„æ˜ å°„å…³ç³»")
    
    async def _start_file_watching(self):
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§"""
        if not self.enable_file_watching:
            return
        
        logger.info("å¯åŠ¨æ–‡ä»¶ç³»ç»Ÿç›‘æ§...")
        
        # åˆ›å»ºæ–‡ä»¶ç›‘æ§å™¨ - ä½¿ç”¨é…ç½®çš„ç›‘æ§ç›®å½•
        self.file_watcher = FileSystemWatcher(settings.vector_watch_directory)
        
        # æ³¨å†Œå›è°ƒå‡½æ•°
        self.file_watcher.add_file_added_callback(self._on_file_added)
        self.file_watcher.add_file_removed_callback(self._on_file_removed)
        self.file_watcher.add_file_modified_callback(self._on_file_modified)
        
        # å¯åŠ¨ç›‘æ§
        self.file_watcher.start()
        
        logger.info(f"æ–‡ä»¶ç³»ç»Ÿç›‘æ§å·²å¯åŠ¨ï¼Œç›‘æ§ç›®å½•: {settings.vector_watch_directory}")
    
    def _on_file_added(self, file_path: str):
        """å¤„ç†æ–‡ä»¶æ·»åŠ äº‹ä»¶"""
        logger.info(f"æ–‡ä»¶æ·»åŠ äº‹ä»¶: {file_path}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†ï¼Œé¿å…é˜»å¡æ–‡ä»¶ç›‘æ§
        self._schedule_async_task(self._handle_file_added(file_path))
    
    def _on_file_removed(self, file_path: str):
        """å¤„ç†æ–‡ä»¶åˆ é™¤äº‹ä»¶"""
        logger.info(f"æ–‡ä»¶åˆ é™¤äº‹ä»¶: {file_path}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†
        self._schedule_async_task(self._handle_file_removed(file_path))
    
    def _on_file_modified(self, file_path: str):
        """å¤„ç†æ–‡ä»¶ä¿®æ”¹äº‹ä»¶"""
        logger.info(f"æ–‡ä»¶ä¿®æ”¹äº‹ä»¶: {file_path}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥å¤„ç†
        self._schedule_async_task(self._handle_file_modified(file_path))
    
    def _schedule_async_task(self, coro):
        """å®‰å…¨åœ°è°ƒåº¦å¼‚æ­¥ä»»åŠ¡"""
        try:
            # å°è¯•è·å–å½“å‰äº‹ä»¶å¾ªç¯
            loop = asyncio.get_running_loop()
            # åœ¨ä¸»çº¿ç¨‹çš„äº‹ä»¶å¾ªç¯ä¸­åˆ›å»ºä»»åŠ¡
            loop.create_task(coro)
        except RuntimeError:
            # å¦‚æœæ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨
            import threading
            import concurrent.futures
            
            def run_async_task():
                try:
                    asyncio.run(coro)
                except Exception as e:
                    logger.error(f"å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                executor.submit(run_async_task)
    
    async def _handle_file_added(self, file_path: str):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶æ·»åŠ """
        try:
            # é˜²æ­¢é‡å¤å¤„ç†
            if file_path in self._processing_files:
                logger.debug(f"æ–‡ä»¶æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡: {file_path}")
                return
            
            with self._lock:
                self._processing_files.add(file_path)
            
            logger.info(f"å¼€å§‹å¤„ç†æ–°æ·»åŠ çš„æ–‡ä»¶: {file_path}")
            
            # åŠ è½½æ–‡æ¡£
            documents = self.document_loader.load_single_file(file_path)
            
            if documents:
                # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                self.add_documents(documents)
                
                # æ›´æ–°æ˜ å°„ - ä½¿ç”¨å®é™…çš„æ–‡æ¡£ID
                with self._lock:
                    # å°è¯•å¤šç§è·¯å¾„æ ¼å¼è·å–æ–‡æ¡£ID
                    normalized_paths = [
                        file_path,
                        str(Path(file_path)),
                        str(Path(file_path).as_posix()),
                        file_path.replace('./', ''),
                        str(Path(file_path).relative_to('.') if file_path.startswith('./') else Path(file_path))
                    ]
                    
                    doc_ids = []
                    for path_variant in normalized_paths:
                        doc_ids = self.get_document_ids_by_source(path_variant)
                        if doc_ids:
                            logger.debug(f"æ‰¾åˆ°æ–‡æ¡£IDä½¿ç”¨è·¯å¾„æ ¼å¼: {path_variant}")
                            break
                    
                    self._file_document_mapping[file_path] = doc_ids
                    self._file_last_modified[file_path] = Path(file_path).stat().st_mtime
                
                # ä¿å­˜å‘é‡å­˜å‚¨
                self.save_vector_store()
                
                logger.info(f"æˆåŠŸæ·»åŠ æ–‡ä»¶åˆ°å‘é‡å­˜å‚¨: {file_path}, æ–‡æ¡£æ•°: {len(documents)}")
            else:
                logger.warning(f"æ–‡ä»¶åŠ è½½å¤±è´¥æˆ–æ— å†…å®¹: {file_path}")
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶æ·»åŠ å¤±è´¥ {file_path}: {str(e)}")
        finally:
            with self._lock:
                self._processing_files.discard(file_path)
    
    async def _handle_file_removed(self, file_path: str):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶åˆ é™¤"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶åˆ é™¤: {file_path}")
            
            # è§„èŒƒåŒ–è·¯å¾„æ ¼å¼ï¼Œå°è¯•å¤šç§æ ¼å¼åŒ¹é…
            normalized_paths = [
                file_path,
                str(Path(file_path)),
                str(Path(file_path).as_posix()),
                file_path.replace('./', ''),
                str(Path(file_path).relative_to('.') if file_path.startswith('./') else Path(file_path))
            ]
            
            success = False
            for path_variant in normalized_paths:
                try:
                    if self.delete_documents_by_source(path_variant):
                        success = True
                        logger.info(f"æˆåŠŸä½¿ç”¨è·¯å¾„æ ¼å¼åˆ é™¤æ–‡æ¡£: {path_variant}")
                        break
                except Exception as e:
                    logger.debug(f"è·¯å¾„æ ¼å¼ {path_variant} åˆ é™¤å¤±è´¥: {str(e)}")
                    continue
            
            if success:
                with self._lock:
                    # æ›´æ–°æ˜ å°„ - åˆ é™¤æ‰€æœ‰å¯èƒ½çš„è·¯å¾„æ ¼å¼
                    for path_variant in normalized_paths:
                        if path_variant in self._file_document_mapping:
                            del self._file_document_mapping[path_variant]
                        self._file_last_modified.pop(path_variant, None)
                
                # ä¿å­˜å‘é‡å­˜å‚¨
                self.save_vector_store()
                
                logger.info(f"æˆåŠŸä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æ–‡ä»¶: {file_path}")
            else:
                logger.warning(f"æ–‡ä»¶å¯èƒ½ä¸å­˜åœ¨äºå‘é‡å­˜å‚¨ä¸­: {file_path}")
                logger.debug(f"å°è¯•çš„è·¯å¾„æ ¼å¼: {normalized_paths}")
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶åˆ é™¤å¤±è´¥ {file_path}: {str(e)}")
    
    async def _handle_file_modified(self, file_path: str):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶ä¿®æ”¹"""
        try:
            # é˜²æ­¢é‡å¤å¤„ç†
            if file_path in self._processing_files:
                logger.debug(f"æ–‡ä»¶æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡: {file_path}")
                return
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«ä¿®æ”¹
            try:
                current_mtime = Path(file_path).stat().st_mtime
                last_mtime = self._file_last_modified.get(file_path, 0)
                
                if abs(current_mtime - last_mtime) < 1:  # 1ç§’å†…çš„ä¿®æ”¹è®¤ä¸ºæ˜¯åŒä¸€æ¬¡
                    logger.debug(f"æ–‡ä»¶ä¿®æ”¹æ—¶é—´å·®å¼‚è¿‡å°ï¼Œè·³è¿‡: {file_path}")
                    return
            except Exception:
                pass  # å¦‚æœè·å–ä¿®æ”¹æ—¶é—´å¤±è´¥ï¼Œç»§ç»­å¤„ç†
            
            with self._lock:
                self._processing_files.add(file_path)
            
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶ä¿®æ”¹: {file_path}")
            
            # å…ˆåˆ é™¤æ—§æ–‡æ¡£
            await self._handle_file_removed(file_path)
            
            # å†æ·»åŠ æ–°æ–‡æ¡£
            await self._handle_file_added(file_path)
            
            logger.info(f"æ–‡ä»¶ä¿®æ”¹å¤„ç†å®Œæˆ: {file_path}")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ä¿®æ”¹å¤±è´¥ {file_path}: {str(e)}")
        finally:
            with self._lock:
                self._processing_files.discard(file_path)
    
    def get_file_document_mapping(self) -> Dict[str, List[str]]:
        """è·å–æ–‡ä»¶åˆ°æ–‡æ¡£çš„æ˜ å°„å…³ç³»"""
        with self._lock:
            return self._file_document_mapping.copy()
    
    def get_processing_files(self) -> Set[str]:
        """è·å–æ­£åœ¨å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        with self._lock:
            return self._processing_files.copy()
    
    async def force_sync_with_filesystem(self):
        """å¼ºåˆ¶ä¸æ–‡ä»¶ç³»ç»ŸåŒæ­¥"""
        logger.info("å¼€å§‹å¼ºåˆ¶åŒæ­¥æ–‡ä»¶ç³»ç»Ÿ...")
        
        try:
            # è·å–å½“å‰æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ‰€æœ‰æ–‡ä»¶
            current_files = set()
            
            # éå†æ‰€æœ‰é…ç½®çš„æ–‡æ¡£ç›®å½•
            for directory_path in settings.document_directories:
                data_dir = Path(directory_path)
                
                if data_dir.exists():
                    logger.info(f"æ­£åœ¨æ‰«æç›®å½•: {directory_path}")
                    for file_path in data_dir.rglob("*"):
                        if file_path.is_file() and file_path.suffix.lower() in ['.pdf', '.docx', '.doc', '.txt', '.md', '.csv', '.xlsx', '.xls']:
                            current_files.add(str(file_path))
                else:
                    logger.warning(f"ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {directory_path}")
            
            # è·å–å‘é‡å­˜å‚¨ä¸­è®°å½•çš„æ–‡ä»¶
            with self._lock:
                stored_files = set(self._file_document_mapping.keys())
            
            # æ‰¾å‡ºéœ€è¦æ·»åŠ çš„æ–‡ä»¶
            files_to_add = current_files - stored_files
            
            # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„æ–‡ä»¶
            files_to_remove = stored_files - current_files
            
            logger.info(f"åŒæ­¥ç»Ÿè®¡ - éœ€è¦æ·»åŠ : {len(files_to_add)}, éœ€è¦åˆ é™¤: {len(files_to_remove)}")
            logger.info(f"æ‰«æçš„ç›®å½•: {settings.document_directories}")
            
            # å¤„ç†åˆ é™¤
            for file_path in files_to_remove:
                await self._handle_file_removed(file_path)
            
            # å¤„ç†æ·»åŠ 
            for file_path in files_to_add:
                await self._handle_file_added(file_path)
            
            logger.info("æ–‡ä»¶ç³»ç»ŸåŒæ­¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç³»ç»ŸåŒæ­¥å¤±è´¥: {str(e)}")
    
    def get_status(self) -> Dict[str, any]:
        """è·å–åŠ¨æ€å‘é‡å­˜å‚¨çŠ¶æ€"""
        with self._lock:
            return {
                "file_watching_enabled": self.enable_file_watching,
                "mcp_enabled": self.enable_mcp,
                "mcp_available": self.mcp_manager.is_available() if self.mcp_manager else False,
                "file_watcher_running": self.file_watcher.is_running if self.file_watcher else False,
                "tracked_files_count": len(self._file_document_mapping),
                "processing_files_count": len(self._processing_files),
                "tracked_files": list(self._file_document_mapping.keys()),
                "processing_files": list(self._processing_files),
                "last_sync_time": datetime.now().isoformat()
            } 